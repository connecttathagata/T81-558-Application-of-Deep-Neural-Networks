{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "        \n",
    "# This function submits an assignment.  You can submit an assignment as much as you like, only the final\n",
    "# submission counts.  The paramaters are as follows:\n",
    "# data - Pandas dataframe output.\n",
    "# key - Your student key that was emailed to you.\n",
    "# no - The assignment class number, should be 1 through 1.\n",
    "# source_file - The full path to your Python or IPYNB file.  This must have \"_class1\" as part of its name.  \n",
    "# .             The number must match your assignment number.  For example \"_class2\" for class assignment #2.\n",
    "def submit(data,key,no,source_file=None):\n",
    "    if source_file is None and '__file__' not in globals(): raise Exception('Must specify a filename when a Jupyter notebook.')\n",
    "    if source_file is None: source_file = __file__\n",
    "    suffix = '_class{}'.format(no)\n",
    "    if suffix not in source_file: raise Exception('{} must be part of the filename.'.format(suffix))\n",
    "    with open(source_file, \"rb\") as image_file:\n",
    "        encoded_python = base64.b64encode(image_file.read()).decode('ascii')\n",
    "    ext = os.path.splitext(source_file)[-1].lower()\n",
    "    if ext not in ['.ipynb','.py']: raise Exception(\"Source file is {} must be .py or .ipynb\".format(ext))\n",
    "    r = requests.post(\"https://api.heatonresearch.com/assignment-submit\",\n",
    "        headers={'x-api-key':key}, json={'csv':base64.b64encode(data.to_csv(index=False).encode('ascii')).decode(\"ascii\"),\n",
    "        'assignment': no, 'ext':ext, 'py':encoded_python})\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success: {}\".format(r.text))\n",
    "    else: print(\"Failure: {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tbanerjee/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=800, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "df = pd.read_csv('../input/train.csv')\n",
    "\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "encode_text_dummy(df, 'manufacturer')\n",
    "\n",
    "n1=df['name'].str.contains('Paperclips').astype(int)\n",
    "n2=df['name'].str.contains('Pencils').astype(int)\n",
    "n3=df['name'].str.contains('Pens').astype(int)\n",
    "n4=df['name'].str.contains('Tablets').astype(int)\n",
    "n5=df['name'].str.contains('Thumbtacks').astype(int)\n",
    "n6=df['name'].str.contains('Paperweights').astype(int)\n",
    "n7=df['name'].str.contains('Stapler').astype(int)\n",
    "n8=df['name'].str.contains('Post').astype(int)\n",
    "\n",
    "df.insert(1,'Paperclips',n1)\n",
    "df.insert(1,'Pencils',n2)\n",
    "df.insert(1,'Pens',n3)\n",
    "df.insert(1,'Tablets',n4)\n",
    "df.insert(1,'Thumbtacks',n5)\n",
    "df.insert(1,'Paperweights',n6)\n",
    "df.insert(1,'Stapler',n7)\n",
    "df.insert(1,'Post',n8)\n",
    "\n",
    "N1=df['name'].str.contains('High').astype(int)\n",
    "N2=df['name'].str.contains('Medium').astype(int)\n",
    "N3=df['name'].str.contains('Large').astype(int)\n",
    "N4=df['name'].str.contains('Small').astype(int)\n",
    "N5=df['name'].str.contains('Tiny').astype(int)\n",
    "\n",
    "df.insert(1,'High',N1)\n",
    "df.insert(1,'Medium',N2)\n",
    "df.insert(1,'Large',N3)\n",
    "df.insert(1,'Small',N4)\n",
    "df.insert(1,'Tiny',N5)\n",
    "\n",
    "C1=df['name'].str.contains('Red').astype(int)\n",
    "C2=df['name'].str.contains('Pink').astype(int)\n",
    "C3=df['name'].str.contains('Black').astype(int)\n",
    "C4=df['name'].str.contains('Green').astype(int)\n",
    "C5=df['name'].str.contains('White').astype(int)\n",
    "C6=df['name'].str.contains('Blue').astype(int)\n",
    "C7=df['name'].str.contains('Brown').astype(int)\n",
    "\n",
    "df.insert(1,'Red',C1)\n",
    "df.insert(1,'Pink',C2)\n",
    "df.insert(1,'Black',C3)\n",
    "df.insert(1,'Green',C4)\n",
    "df.insert(1,'White',C5)\n",
    "df.insert(1,'Blue',C6)\n",
    "df.insert(1,'Brown',C7)\n",
    "\n",
    "M1=df['name'].str.contains('Generic').astype(int)\n",
    "df.insert(1,'Generic',M1)\n",
    "\n",
    "\n",
    "df.drop('name', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df['cost']=np.log(df['cost'])\n",
    "df['pack']=np.log(df['pack'])\n",
    "df['weight']=np.log(df['weight'])\n",
    "\n",
    "df['height']=np.log(df['height'])\n",
    "df['width']=np.log(df['width'])\n",
    "df['length']=np.log(df['length'])\n",
    "\n",
    "\n",
    "\n",
    "x,y = to_xy(df,\"cost\")\n",
    "\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(n_estimators=4000, learning_rate=0.05,\n",
    "                                   max_depth=5, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_gbr.fit(x,y)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=1400,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_xgb.fit(x,y)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "model_lgb.fit(x,y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = pd.read_csv('../input/test.csv')\n",
    "\n",
    "ids = tdf['id']\n",
    "tdf.drop('id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "encode_text_dummy(tdf, 'manufacturer')\n",
    "\n",
    "n1=tdf['name'].str.contains('Paperclips').astype(int)\n",
    "n2=tdf['name'].str.contains('Pencils').astype(int)\n",
    "n3=tdf['name'].str.contains('Pens').astype(int)\n",
    "n4=tdf['name'].str.contains('Tablets').astype(int)\n",
    "n5=tdf['name'].str.contains('Thumbtacks').astype(int)\n",
    "n6=tdf['name'].str.contains('Paperweights').astype(int)\n",
    "n7=tdf['name'].str.contains('Stapler').astype(int)\n",
    "n8=tdf['name'].str.contains('Post').astype(int)\n",
    "\n",
    "tdf.insert(1,'Paperclips',n1)\n",
    "tdf.insert(1,'Pencils',n2)\n",
    "tdf.insert(1,'Pens',n3)\n",
    "tdf.insert(1,'Tablets',n4)\n",
    "tdf.insert(1,'Thumbtacks',n5)\n",
    "tdf.insert(1,'Paperweights',n6)\n",
    "tdf.insert(1,'Stapler',n7)\n",
    "tdf.insert(1,'Post',n8)\n",
    "\n",
    "N1=tdf['name'].str.contains('High').astype(int)\n",
    "N2=tdf['name'].str.contains('Medium').astype(int)\n",
    "N3=tdf['name'].str.contains('Large').astype(int)\n",
    "N4=tdf['name'].str.contains('Small').astype(int)\n",
    "N5=tdf['name'].str.contains('Tiny').astype(int)\n",
    "\n",
    "tdf.insert(1,'High',N1)\n",
    "tdf.insert(1,'Medium',N2)\n",
    "tdf.insert(1,'Large',N3)\n",
    "tdf.insert(1,'Small',N4)\n",
    "tdf.insert(1,'Tiny',N5)\n",
    "\n",
    "C1=tdf['name'].str.contains('Red').astype(int)\n",
    "C2=tdf['name'].str.contains('Pink').astype(int)\n",
    "C3=tdf['name'].str.contains('Black').astype(int)\n",
    "C4=tdf['name'].str.contains('Green').astype(int)\n",
    "C5=tdf['name'].str.contains('White').astype(int)\n",
    "C6=tdf['name'].str.contains('Blue').astype(int)\n",
    "C7=tdf['name'].str.contains('Brown').astype(int)\n",
    "\n",
    "tdf.insert(1,'Red',C1)\n",
    "tdf.insert(1,'Pink',C2)\n",
    "tdf.insert(1,'Black',C3)\n",
    "tdf.insert(1,'Green',C4)\n",
    "tdf.insert(1,'White',C5)\n",
    "tdf.insert(1,'Blue',C6)\n",
    "tdf.insert(1,'Brown',C7)\n",
    "\n",
    "M1=tdf['name'].str.contains('Generic').astype(int)\n",
    "tdf.insert(1,'Generic',M1)\n",
    "\n",
    "tdf.drop('name', axis=1, inplace=True)\n",
    "\n",
    "tdf['pack']=np.log(tdf['pack'])\n",
    "tdf['weight']=np.log(tdf['weight'])\n",
    "\n",
    "tdf['height']=np.log(tdf['height'])\n",
    "tdf['width']=np.log(tdf['width'])\n",
    "tdf['length']=np.log(tdf['length'])\n",
    "\n",
    "x_test = tdf.as_matrix().astype(np.float32)\n",
    "\n",
    "\n",
    "tmodel_gbr = model_gbr.predict(x_test)\n",
    "pred1=np.exp(tmodel_gbr)\n",
    "\n",
    "tmodel_xgb=model_xgb.predict(x_test)\n",
    "pred2=np.exp(tmodel_xgb)\n",
    "\n",
    "tmodel_lgb=model_lgb.predict(x_test)\n",
    "pred3=np.exp(tmodel_lgb)\n",
    "\n",
    "pred=pred1*0.75+pred2*0.15+pred3*0.1\n",
    "\n",
    "df_submit = pd.DataFrame(pred)\n",
    "df_submit.insert(0,'id',ids)\n",
    "df_submit.columns = ['id','cost']\n",
    "\n",
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
